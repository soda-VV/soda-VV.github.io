<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>『工程记录』本地部署chatglm3-6b</title>
      <link href="/posts/d5ac.html"/>
      <url>/posts/d5ac.html</url>
      
        <content type="html"><![CDATA[<h1 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h1><p>ChatGLM3-6B发布于2023年10月27日，是目前中文能力排名最高的开源LLM模型。对应项目的github地址：<a href="https://github.com/THUDM/ChatGLM3">https://github.com/THUDM/ChatGLM3</a></p><p>实验室项目中需要本地部署大模型用于新闻文本摘要任务，因为预测速度的要求及硬件条件的限制，比较后选择该模型的6B版本。</p><p>本文为在linux环境下的chatglm3-6b的本地部署工程记录。</p><h1 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h1><h2 id="CUDA与CUDNN"><a href="#CUDA与CUDNN" class="headerlink" title="CUDA与CUDNN"></a>CUDA与CUDNN</h2><p>深度学习任务中，当使用GPU进行训练的时候，需要安装英伟达提供的驱动和显卡对应的cuda、cudnn。</p><h3 id="什么是cuda和cudnn？"><a href="#什么是cuda和cudnn？" class="headerlink" title="什么是cuda和cudnn？"></a>什么是cuda和cudnn？</h3><ul><li>CUDA(ComputeUnified Device Architecture)：是一种由NVIDIA推出的通用并行计算架构，该架构使GPU能够解决复杂的计算问题。</li><li>CUDNN：是用于深度神经网络的GPU加速库。它强调性能、易用性和低内存开销。NVIDIA cuDNN可以集成到更高级别的机器学习框架中，如谷歌的Tensorflow、torch</li></ul><p>需要注意的是，cuda、cudnn和我们常用的深度学习框架（如tensorflow-gpu、torch-gpu）之间的版本存在对应关系，如果配置错误则代码不能运行。</p><p>我们首先查看服务器上的显卡相关信息，在命令行输入：</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure><p>可以看到显卡支持的最高CUDA版本为11.8（<strong>注意这里是显卡支持的最高CUDA版本，而并不是目前系统安装的CUDA版本</strong>！）</p><p>同时可以看到这台服务器上装载了两个显存大小都为24G的GPU 0和1，其中GPU 0的显存几乎已经完全占满。</p><p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240122191115501.png" alt="image-20240122191115501"></p><p>输入以下命令，查看是否已经安装了cuda：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc -V</span><br></pre></td></tr></table></figure><p>如果出现 <code>Command &#39;nvcc&#39; not found, but can be installed with:</code> 的输出，说明该服务器的系统并没有安装cuda</p><p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240122193539449.png" alt="image-20240122193539449"></p><p>可以看到我这台服务器上面已经安装了cuda，版本为11.8，所以并不需要进行cuda和cudnn的安装</p><p><strong>如果你和我一样已经安装了cuda和cudnn，请跳转到下一部分，进行相对应torch-gpu的安装</strong></p><h3 id="cuda和cudnn的安装"><a href="#cuda和cudnn的安装" class="headerlink" title="cuda和cudnn的安装"></a>cuda和cudnn的安装</h3><p>这一部分我并没有进行，考虑后续有没有机会补上。</p><p>首先要创建一个Python ≥ 3.10的虚拟环境（chatglm3-6b要求）。</p><p>要注意虚拟环境中的cuda和系统中的cuda的区别，我这里的cuda就是直接安装在系统中，所有的虚拟环境都可以使用系统中安装的cuda</p><p>在虚拟环境中安装并使用其他版本的cuda，参考：<a href="https://blog.csdn.net/2301_80501457/article/details/134191613">https://blog.csdn.net/2301_80501457/article/details/134191613</a></p><blockquote><p>要注意：如果你的cuda是直接安装在虚拟环境中，并非直接安装在系统中。是无法使用ncvv -V命令验证是否安装成功的，ncvv -V是通过系统变量来直接查询，而安装在虚拟环境中并没有直接建立系统变量。验证方法可以见上面链接的最后一部分。</p></blockquote><h2 id="pytorch-gpu的安装"><a href="#pytorch-gpu的安装" class="headerlink" title="pytorch-gpu的安装"></a>pytorch-gpu的安装</h2><p>pytorch-gpu的版本同样必须依赖于CUDA的版本。</p><p>注意，<code>conda install pytorch</code> 命令安装的是torch CPU版本，但是我们要使用GPU进行训练</p><p>在<a href="https://pytorch.org/get-started/previous-versions/">pytorch官网</a>上获取cuda11.8对应版本torch的安装命令</p><p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240122103348292.png" alt="image-20240122103348292"></p><p>复制对应的命令，执行安装</p><p>安装完成后编写程序如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="comment"># 如果pytorch安装成功即可导入</span></span><br><span class="line"><span class="built_in">print</span>(torch.cuda.is_available()) <span class="comment"># 查看CUDA是否可用</span></span><br><span class="line"><span class="built_in">print</span>(torch.cuda.device_count()) <span class="comment"># 查看可用的CUDA数量</span></span><br><span class="line"><span class="built_in">print</span>(torch.version.cuda) <span class="comment"># 查看CUDA版本</span></span><br></pre></td></tr></table></figure><p>运行结果如下：</p><p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240122195902702.png" alt="image-20240122195902702"></p><p>已经配置成功</p><h2 id="安装剩余依赖"><a href="#安装剩余依赖" class="headerlink" title="安装剩余依赖"></a>安装剩余依赖</h2><p>clone下来ChatGLM3项目</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/THUDM/ChatGLM3.git</span><br></pre></td></tr></table></figure><blockquote><p>超时的话挂梯子设置代理，使用服务器没办法挂梯子的话直接在github下载zip上传到服务器上</p></blockquote><p>由于已经安装了torch，所以我们要<strong>删掉项目目录下requirements.txt中的 <em>torch&gt;&#x3D;2.1.0</em> 一行</strong></p><p>并使用 pip 安装剩余的依赖</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><blockquote><p>这里一定要删除torch相关依赖行！！不然会自动给你下载冲突的torch版本，后续项目报错</p></blockquote><h1 id="下载ChatGLM3-6b模型及参数"><a href="#下载ChatGLM3-6b模型及参数" class="headerlink" title="下载ChatGLM3-6b模型及参数"></a>下载ChatGLM3-6b模型及参数</h1><h2 id="方法1：从-Hugging-Face-Hub-下载模型"><a href="#方法1：从-Hugging-Face-Hub-下载模型" class="headerlink" title="方法1：从 Hugging Face Hub 下载模型"></a>方法1：从 Hugging Face Hub 下载模型</h2><p>首先需要安装Git LFS，否则会出现模型中的大型文件下载不完整的情况。</p><p>输入以下命令</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git lfs install</span><br></pre></td></tr></table></figure><p>若显示 <code>Git LFS initialized</code> ，说明已经安装。</p><p>可以使用git进行克隆，运行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://huggingface.co/THUDM/chatglm3-6b</span><br></pre></td></tr></table></figure><p>如果下载比较慢或者超时，可以直接去<a href="https://huggingface.co/THUDM/chatglm3-6b">官网</a>下载模型再上传到服务器</p><h2 id="方法2：从-ModelScope-下载模型（推荐）"><a href="#方法2：从-ModelScope-下载模型（推荐）" class="headerlink" title="方法2：从 ModelScope 下载模型（推荐）"></a>方法2：从 ModelScope 下载模型（推荐）</h2><p>同样也可以在国内魔搭社区下载，一般不会超时</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git lfs install</span><br><span class="line">git clone https://www.modelscope.cn/ZhipuAI/chatglm3-6b.git</span><br></pre></td></tr></table></figure><blockquote><p>注意，这里一定要<code>git lfs install</code>，虽然我查了这个命令仅仅只是用于验证是否安装了git lfs，但是我第一次从魔搭git clone的时候，没有使用这个命令产生了大文件下载不完整的情况，加上后没有再出现这样的问题，所以建议还是加上吧。</p></blockquote><h1 id="代码调用模型进行预测"><a href="#代码调用模型进行预测" class="headerlink" title="代码调用模型进行预测"></a>代码调用模型进行预测</h1><p>下载完成模型及参数文件后，我们将所有的文件放在一个文件夹<code>chatglm3-6b</code>里，然后将其放在项目文件夹ChatGLM3-main下</p><p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240122202232999.png" alt="image-20240122202232999"></p><p>然后我们可以在项目主目录下创建test.py文件，通过如下代码调用 ChatGLM 模型来生成对话：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModel</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;chatglm3-6b/&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModel.from_pretrained(<span class="string">&quot;chatglm3-6b/&quot;</span>, trust_remote_code=<span class="literal">True</span>, device=<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">model = model.<span class="built_in">eval</span>()</span><br><span class="line">response, history = model.chat(tokenizer, <span class="string">&quot;你好&quot;</span>, history=[])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;回答：&quot;</span>,response)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要根据上下文的情况下</span></span><br><span class="line"><span class="comment"># response, history = model.chat(tokenizer, &quot;晚上睡不着应该怎么办&quot;, history=history)</span></span><br><span class="line"><span class="comment"># print(response)</span></span><br></pre></td></tr></table></figure><blockquote><p>这里使用”chatglm3-6b&#x2F;“就是加载了该文件夹下的本地模型及文件</p></blockquote><p>在命令行输入，执行代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=1 python test.py</span><br></pre></td></tr></table></figure><blockquote><p>注意，这里的 <code>CUDA_VISIBLE_DEVICES=1</code> 的作用是指定使用 GPU 1 来跑模型，因为我们前面已经通过 <code>nvidia-smi</code> 知道 GPU 0 的显存已经基本占满了</p><p>cuda指定GPU、设置多GPU的方法：<a href="https://blog.csdn.net/OneQuestionADay/article/details/111691486">https://blog.csdn.net/OneQuestionADay/article/details/111691486</a></p></blockquote><h1 id="构建网页版demo"><a href="#构建网页版demo" class="headerlink" title="构建网页版demo"></a>构建网页版demo</h1><p>官方教程：<a href="https://github.com/THUDM/ChatGLM3/blob/main/composite_demo/README.md">https://github.com/THUDM/ChatGLM3/blob/main/composite_demo/README.md</a></p><p>安装 Jupyter 内核：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ipython kernel install --name chatglm3-demo --user</span><br></pre></td></tr></table></figure><p>因为我们是本地下载的模型，所以需要先设置环境变量 <code>MODEL_PATH</code> 来指定从本地加载模型</p><p>直接在终端中输入如下命令即可</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export MODEL_PATH=//home/xjb/event/ChatGLM3-main/chatglm3-6b</span><br></pre></td></tr></table></figure><p>然后输入命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=1 streamlit run composite_demo/main.py</span><br></pre></td></tr></table></figure><blockquote><p>注意这里一样要在命令的前面加上 <code>CUDA_VISIBLE_DEVICES=1</code> 来设置使用的GPU</p></blockquote><p>运行结果：</p><p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240122204008879.png" alt="image-20240122204008879"></p><p>如果你部署在了服务器上而不是本机，访问对应的network url既可使用网页demo</p><p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240122204313569.png" alt="image-20240122204313569"></p><p>现在你就拥有一个属于自己的本地大模型啦，因为该模型较轻量级，预测的速度还是比较快的，在未经过微调的情况下我直接将其用于新闻文本的摘要任务，大概1-2秒可以生成一篇文本的摘要，效果也比较不错，但是偶尔会出现中英文乱码的情况。</p><p>你也可以对其进行进一步的领域微调，使其更适合你的任务。</p><h1 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h1><ul><li><p><a href="https://blog.csdn.net/qq_43705697/article/details/121618276">https://blog.csdn.net/qq_43705697/article/details/121618276</a></p></li><li><p><a href="https://blog.csdn.net/2301_80501457/article/details/134191613">https://blog.csdn.net/2301_80501457/article/details/134191613</a></p></li><li><p><a href="https://blog.csdn.net/m0_65814643/article/details/134510135?spm=1001.2014.3001.5501">https://blog.csdn.net/m0_65814643/article/details/134510135?spm=1001.2014.3001.5501</a></p></li><li><p><a href="https://blog.csdn.net/m0_65814643/article/details/134563092">https://blog.csdn.net/m0_65814643/article/details/134563092</a></p></li><li><p><a href="https://blog.csdn.net/OneQuestionADay/article/details/111691486">https://blog.csdn.net/OneQuestionADay/article/details/111691486</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 工程记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>『linux』ubuntu修改python版本</title>
      <link href="/posts/3bee.html"/>
      <url>/posts/3bee.html</url>
      
        <content type="html"><![CDATA[<p><strong>motivation：</strong></p><p>服务器系统为：ubuntu20.04</p><p>查看系统中自带的python版本，可以发现系统自带的python3版本为python3.8。</p><p>因为我们需要在服务器部署的项目依赖python版本&gt;&#x3D;3.9，与Python3.8不兼容，所以需要安装python3.9。</p><p>本文详细记录在Ubuntu20.04（其他版本步骤也一样）上编译安装指定版本Python解释器的过程和可能遇到的问题，及多个Python版本并存的使用方法。</p><h1 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h1><p>初次安装的时候，没有执行这个步骤，后续踩了比较多的坑，参考网上的文章后执行此步骤可以解决大部分的问题。</p><h2 id="更新系统软件"><a href="#更新系统软件" class="headerlink" title="更新系统软件"></a>更新系统软件</h2><p>在正式开始之前，建议首先检查系统软件是否均为最新，并更新到最新版本。</p><p>打开一个终端，输入以下命令：</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 刷新软件包目录</span><br><span class="line">sudo apt update</span><br><span class="line"># 列出当前可用的更新</span><br><span class="line">sudo apt list --upgradable</span><br><span class="line"># 如上一步提示有可以更新的项目，则执行更新</span><br><span class="line">sudo apt upgrade</span><br></pre></td></tr></table></figure><p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240111170901524.png" alt="image-20240111170901524"></p><h2 id="安装GCC编译器"><a href="#安装GCC编译器" class="headerlink" title="安装GCC编译器"></a>安装GCC编译器</h2><p>打开一个终端，使用 apt 安装 GCC 编译器：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装 GCC 编译器</span></span><br><span class="line">sudo apt install gcc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查安装是否成功</span></span><br><span class="line">gcc -v</span><br><span class="line"><span class="comment"># 若显示出 GCC 版本则成功</span></span><br></pre></td></tr></table></figure><h2 id="安装其他依赖"><a href="#安装其他依赖" class="headerlink" title="安装其他依赖"></a>安装其他依赖</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 刷新软件包目录</span></span><br><span class="line">sudo apt update</span><br><span class="line"><span class="comment"># 安装依赖</span></span><br><span class="line">sudo apt install build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev libbz2-dev liblzma-dev sqlite3 libsqlite3-dev tk-dev uuid-dev libgdbm-compat-dev</span><br></pre></td></tr></table></figure><p>注意，Python 的部分功能依赖于对应的库（如 OpenSSL、SQLite3、LZMA 等），如果在编译时未能找到这些库，仍然可能完成编译。此时的 Python 解释器看似可以工作，但在需要使用特定功能时就会出问题。例如 OpenSSL 出现问题会导致无法正常使用 pip。故建议按本节提示安装所有可选依赖项。</p><h1 id="查看系统自带python3版本"><a href="#查看系统自带python3版本" class="headerlink" title="查看系统自带python3版本"></a>查看系统自带python3版本</h1><p>在系统终端中输入：</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls -l /usr/bin | grep python</span><br></pre></td></tr></table></figure><p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240111165216118.png" alt="image-20240111165216118"></p><p>可以看出使用系统的Python3指向版本python3.8</p><p>输入python3 –version可以查看详细版本信息</p><p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240111165421131.png" alt="image-20240111165421131"></p><h1 id="下载安装python3-9"><a href="#下载安装python3-9" class="headerlink" title="下载安装python3.9"></a>下载安装python3.9</h1><h2 id="下载与解压"><a href="#下载与解压" class="headerlink" title="下载与解压"></a>下载与解压</h2><p>在<a href="https://www.python.org/ftp/python/%E4%B8%8B%E6%89%BE%E5%88%B0%E6%88%91%E4%BB%AC%E6%83%B3%E8%A6%81%E5%AE%89%E8%A3%85%E7%9A%84python3.9%E7%89%88%E6%9C%AC%E7%9A%84tgz%E5%AE%89%E8%A3%85%E5%8C%85">https://www.python.org/ftp/python/下找到我们想要安装的python3.9版本的tgz安装包</a></p><p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240111165757616.png" alt="image-20240111165757616"></p><p>在终端中通过wget命令下载我们选择的安装包</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo wget https://www.python.org/ftp/python/<span class="number">3</span>.<span class="number">9</span>.<span class="number">0</span>/Python-<span class="number">3</span>.<span class="number">9</span>.<span class="number">0</span>.tgz</span><br></pre></td></tr></table></figure><p>解压安装包并进入目录（我这里选择解压到根目录下，读者可以自行选择解压路径）</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo tar -zxvf Python-<span class="number">3</span>.<span class="number">9</span>.<span class="number">0</span>.tgz -C ~</span><br><span class="line"><span class="built_in">cd</span> Python-<span class="number">3</span>.<span class="number">8</span>.<span class="number">5</span></span><br></pre></td></tr></table></figure><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>在Python-3.8.5目录下：</p><p>配置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ./configure</span><br></pre></td></tr></table></figure><p>编译：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make</span><br></pre></td></tr></table></figure><p>安装：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo make altinstall</span><br></pre></td></tr></table></figure><blockquote><p>这里使用应使用 <code>altinstall</code> 而不是 <code>install</code> 。</p><p>二者的一个重要区别在于，后者会创建符号链接，将 <code>python3</code> 等命令链接到正在安装的新版本 Python 3 上，这可能会破坏系统。更多信息请参阅当前目录下的 <code>README.rst</code> 文件。</p></blockquote><h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><p><strong>有的文章可能会在这里让你删除掉原有的python环境或者是修改python3命令指向新安装的python3.8环境，由于 Ubuntu 系统、安装的其他软件等很可能会依赖于系统原有的python环境等原因，都不要进行此类修改！！！</strong></p><p>下面对安装完成的python环境进行测试</p><p>在命令行输入python3.9（注意不要分开），可以看到已经成功安装了。</p><p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240111172737068.png" alt="image-20240111172737068"></p><p>如果要在python3.9环境下安装库的话，使用pip3.9即可</p><p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240111173117603.png" alt="image-20240111173117603"></p><p>后续如果想用python3.9环境来执行代码的话，在前面加上python3.9即可</p><p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240111173227176.png" alt="image-20240111173227176"></p><h1 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h1><ul><li><a href="https://zhuanlan.zhihu.com/p/506491209">https://zhuanlan.zhihu.com/p/506491209</a></li><li></li></ul>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> unbuntu </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>『成都』成都美食探店</title>
      <link href="/posts/1e32.html"/>
      <url>/posts/1e32.html</url>
      
        <content type="html"><![CDATA[<p>探店齁逼多，真假VV说。</p><p>已经在成都待了5年了，未来还有两年甚至也可能更久。这么长时间吃过的店铺也有不少了，这几天又有朋友要来成都找我玩，因为马上期末的缘故没有太多时间陪他们，就整理了一份这几年吃过的一些比较好吃的店铺，希望他们可以玩的开心！</p><h1 id="川菜"><a href="#川菜" class="headerlink" title="川菜"></a>川菜</h1><h2 id="鸡毛店"><a href="#鸡毛店" class="headerlink" title="鸡毛店"></a>鸡毛店</h2><ul><li><p>地点：天府宏</p></li><li><p>人均：50</p></li><li><p>评价：去过好几次了，味道很不错，注意有的店也叫鸡毛店但不一样，推荐天府宏的</p></li><li><p>推荐菜：</p><ul><li><p>蒜蓉生焖虾：必点，非常好吃</p></li><li><p>鸡丝凉面</p></li></ul></li></ul><h2 id="明婷饭店"><a href="#明婷饭店" class="headerlink" title="明婷饭店"></a>明婷饭店</h2><ul><li>地点：去过两个分店，都还可以</li><li>人均：60</li><li>评价：便宜，味道很不错，性价比高。</li><li>推荐菜：<ul><li>脑花豆腐：必点，特别下饭</li><li>奇香排骨：招牌</li></ul></li></ul><h2 id="陈麻婆豆腐"><a href="#陈麻婆豆腐" class="headerlink" title="陈麻婆豆腐"></a>陈麻婆豆腐</h2><ul><li>地点：总店在杜甫草堂对面</li><li>人均：60</li><li>评价：麻婆豆腐非常好吃，其他菜一般般。建议去总店！</li><li>推荐菜：<ul><li>麻婆豆腐：非遗</li></ul></li></ul><h2 id="永乐饭店"><a href="#永乐饭店" class="headerlink" title="永乐饭店"></a>永乐饭店</h2><ul><li>地点：武侯区</li><li>人均：50</li><li>评价：便宜好吃，网上评价都比较好，我还没吃过</li><li>推荐菜：<ul><li>照着招牌点就行</li></ul></li></ul><h1 id="翘脚牛肉"><a href="#翘脚牛肉" class="headerlink" title="翘脚牛肉"></a>翘脚牛肉</h1><h2 id="冯四嬢翘脚牛肉"><a href="#冯四嬢翘脚牛肉" class="headerlink" title="冯四嬢翘脚牛肉"></a>冯四嬢翘脚牛肉</h2><ul><li>地点：世豪广场</li><li>人均：30</li><li>评价：上次朋友来成都玩吃过一次，感觉比较好吃，牛肉很新鲜。</li><li>推荐菜：<ul><li>翘脚牛肉</li><li>鲜血旺</li><li>粉蒸牛肉</li></ul></li></ul><h1 id="烧烤"><a href="#烧烤" class="headerlink" title="烧烤"></a>烧烤</h1><h2 id="新奥尔良烧烤"><a href="#新奥尔良烧烤" class="headerlink" title="新奥尔良烧烤"></a>新奥尔良烧烤</h2><ul><li>地点：（这家店地图上搜不到，可以搜索生火火地摊小炉子烧烤，再旁边），在玉林西路</li><li>人均：60</li><li>评价：烤翅神中神，其他的也不错</li><li>推荐菜：<ul><li>烤翅</li><li>让老板娘帮着拿，都不错</li></ul></li></ul><h1 id="美蛙鱼"><a href="#美蛙鱼" class="headerlink" title="美蛙鱼"></a>美蛙鱼</h1><h2 id="味之绝"><a href="#味之绝" class="headerlink" title="味之绝"></a>味之绝</h2><ul><li>地点：连锁店，哪里都有</li><li>人均：60</li><li>评价：味道不错</li></ul><h1 id="冒烤鸭"><a href="#冒烤鸭" class="headerlink" title="冒烤鸭"></a>冒烤鸭</h1><h2 id="回味冒烤鸭"><a href="#回味冒烤鸭" class="headerlink" title="回味冒烤鸭"></a>回味冒烤鸭</h2><ul><li>地点：回味冒烤鸭（川大店）</li><li>人均：20</li><li>评价：价格便宜，好吃</li></ul><h1 id="火锅"><a href="#火锅" class="headerlink" title="火锅"></a>火锅</h1><p>感觉火锅味道都差不多？</p><h2 id="矮板凳"><a href="#矮板凳" class="headerlink" title="矮板凳"></a>矮板凳</h2><ul><li>地点：连锁，哪里都有</li><li>人均：70</li><li>评价：实验室聚餐去吃过几次，味道还可以，价格也还行</li></ul><h2 id="星鸿社"><a href="#星鸿社" class="headerlink" title="星鸿社"></a>星鸿社</h2><ul><li>地点：建设路&#x2F;电子科技大学</li><li>人均：100+</li><li>评价：一家重庆老火锅。上次朋友来玩一起去吃过，味道很好，但是价格比较贵。</li></ul><h1 id="串串"><a href="#串串" class="headerlink" title="串串"></a>串串</h1><h2 id="六年二班串串"><a href="#六年二班串串" class="headerlink" title="六年二班串串"></a>六年二班串串</h2><ul><li><p>地点：人民公园店</p></li><li><p>人均：40</p></li><li><p>评价：本地人舍友推荐，还没去过</p></li></ul><h1 id="小龙虾"><a href="#小龙虾" class="headerlink" title="小龙虾"></a>小龙虾</h1><h2 id="龙虾一绝"><a href="#龙虾一绝" class="headerlink" title="龙虾一绝"></a>龙虾一绝</h2><ul><li>地点：玉林西路</li><li>人均：70</li><li>评价：本地人舍友推荐的一家龙虾店，去吃过一次，价格较贵，感觉一般般，也有可能是我点的是微辣的原因。</li></ul><h1 id="小吃"><a href="#小吃" class="headerlink" title="小吃"></a>小吃</h1><h2 id="建设北路"><a href="#建设北路" class="headerlink" title="建设北路"></a>建设北路</h2><ul><li><p>烤苕皮</p></li><li><p>油条糯米糍（不叫这个，具体叫啥忘了）：个人觉得非常好吃，每次去都买一个肉松味的。</p></li><li><p>冰粉：必吃，一般吃火锅点一个。</p></li><li><p>冰豆花</p></li><li><p>蛋烘糕</p></li><li><p>成都市很好吃的火锅粉：很多人说超好吃，但我还没去吃过。推荐脑花、火锅粉。</p></li><li><p>钵钵鸡</p></li><li><p>担担面</p></li></ul><h2 id="宽菜日记"><a href="#宽菜日记" class="headerlink" title="宽菜日记"></a>宽菜日记</h2><ul><li>推荐菜：<ul><li>甜皮鸭：都说很好吃，还没吃过，有机会尝试下。建议买来后1-2小时内吃，放久了就不好吃了。</li></ul></li></ul><h2 id="蹄花"><a href="#蹄花" class="headerlink" title="蹄花"></a>蹄花</h2><ul><li><p>吴氏蹄花：川大望江校区附近，学长学姐祖传的老店，个人觉得比较便宜而且好吃，推荐雪豆蹄花汤，可以顺便点一个他们的铁板烧，很好吃。</p></li><li><p>无名蹄花：大众点评必吃榜上的，没吃过</p></li></ul><h1 id="不推荐"><a href="#不推荐" class="headerlink" title="不推荐"></a>不推荐</h1><h2 id="陶德砂锅"><a href="#陶德砂锅" class="headerlink" title="陶德砂锅"></a>陶德砂锅</h2><p>网红店。很一般，本地人都不推荐的店</p><h2 id="烤匠"><a href="#烤匠" class="headerlink" title="烤匠"></a>烤匠</h2><p>本人去过好几次，感觉也是比较一般，而且偏贵，推荐菜：芝士烤红薯</p><h2 id="冒椒火辣"><a href="#冒椒火辣" class="headerlink" title="冒椒火辣"></a>冒椒火辣</h2><p>网红店。小菜很好吃（脑花好吃），但是串串感觉味道很一般，偏贵</p><h2 id="饕林餐厅"><a href="#饕林餐厅" class="headerlink" title="饕林餐厅"></a>饕林餐厅</h2><p>网红店。个人觉得很一般，而且偏贵。-</p>]]></content>
      
      
      <categories>
          
          <category> 生活 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 美食 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>『SEO优化』：Hexo-abbrlink插件生成hexo永久固定链接</title>
      <link href="/posts/1e78.html"/>
      <url>/posts/1e78.html</url>
      
        <content type="html"><![CDATA[<h1 id="hexo默认url格式存在的问题"><a href="#hexo默认url格式存在的问题" class="headerlink" title="hexo默认url格式存在的问题"></a>hexo默认url格式存在的问题</h1><p>Hexo在生成博客文章链接时，默认的静态URL格式是 <code>:year/:month/:day/:title</code>，也就是按照年、月、日、标题格式来生成固定链接的，如<code>http://xxx.yy.com/2020/07/06/hello-world</code>。</p><p><strong>这样的话就会存在一些问题：</strong></p><ul><li><p>如果你的标题是中文的话，你的URL链接就会包含中文，例如：</p><p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20231204194948770.png" alt="image-20231204194948770"></p><p>这样的url路径复制后会将中文变成一大堆字符串编码：<a href="https://cnhuazhu.top/butterfly/2023/03/01/Flask/%E9%83%A8%E7%BD%B2Flask%E9%A1%B9%E7%9B%AE%E8%87%B3%E8%BF%9C%E7%A8%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%AD%E7%9A%84Docker%E5%AE%B9%E5%99%A8%E5%86%85/">https://cnhuazhu.top/butterfly/2023/03/01/Flask/%E9%83%A8%E7%BD%B2Flask%E9%A1%B9%E7%9B%AE%E8%87%B3%E8%BF%9C%E7%A8%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%AD%E7%9A%84Docker%E5%AE%B9%E5%99%A8%E5%86%85/</a> </p><p>如果你在其它地方用了你自己这篇文章的url链接，偶然你又修改了该文章的标题，那<strong>这个url链接就会失效</strong>。</p></li><li><p>这样的url格式中年月日都会有分隔符，生成的链接会成为一个四级目录，<strong>对于搜索引擎来说并不是很友好。</strong></p><blockquote><p>百度蜘蛛在抓取网页时遵循以下规则：</p><p>网页的抓取频率与其权重和信用度成正比，尤其是对网站的首页和内页。通常，蜘蛛首先抓取网站的首页，因为首页通常具有更高的权重，而且大多数链接都指向首页。然后，通过首页抓取网站的内页，但并非所有内页都会被蜘蛛抓取。</p><p>搜索引擎认为对于一般的中小型站点，3层结构足以包含所有内容，因此蜘蛛主要抓取前三层的内容。超过三层的内容被认为相对不重要，因此蜘蛛不会经常爬取这些内容。因此，在设计链接结构时，最好将permalink后面的层级控制在2个斜杠以内。</p></blockquote></li></ul><h1 id="hexo-abbrlink的安装配置"><a href="#hexo-abbrlink的安装配置" class="headerlink" title="hexo-abbrlink的安装配置"></a><strong>hexo-abbrlink的安装配置</strong></h1><p>为了解决上面提到的问题，我们使用Hexo的插件<strong>hexo-abbrlink</strong>，它能将 Hexo 生成的永久链接转化为一个固定的随机值，极大的缩短了永久链接的长度。一旦生成一个随机值，之后对文章的标题或者时间进行任何修改，这个随机的 abbrlink 是不会发生任何变化的，也为 Hexo 的维护提供了便利。</p><p>博客目录下执行命令安装：</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$npm install hexo-abbrlink --save </span><br></pre></td></tr></table></figure><p>修改<code>_config.yml</code>文件中的配置项（记得把原来的<code>permalink:</code>删除掉）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#设置永久链接</span><br><span class="line">permalink: posts/:abbrlink.html  </span><br><span class="line">abbrlink:</span><br><span class="line">    alg: crc16   #算法： crc16(default) and crc32</span><br><span class="line">    rep: dec     #进制： dec(default) and hex</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：</p><blockquote><p>参考文章<a href="https://zhuanlan.zhihu.com/p/169492685">https://zhuanlan.zhihu.com/p/169492685</a></p><p>在生成之前就要改好算法和形式。不然后面再改的话会导致链接不统一。我就踩过坑，但后来是一个文章一个文章删除<code>abbrlink:</code>的值才改过来的，还好我文章不是很多。</p><p>话说最近看到了Hexo-abbrlink2这个插件，可以从1.html开始编。</p><p>我试用了一下，会导致文章重复，正在研究如何解决。如果能从1开始编写的话，这样更容易看得出写了多少篇文章，当前上传和修改的是哪一篇。后面再研究。</p><p>PS：刚使用这个插件后，阅读人数和评论都会变为0，介意慎用！综上所述，这插件适合新站！</p></blockquote><h1 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h1><p>侵权删：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/112318081">https://zhuanlan.zhihu.com/p/112318081</a></li><li><a href="https://zhuanlan.zhihu.com/p/169492685">https://zhuanlan.zhihu.com/p/169492685</a></li><li><a href="https://cloud.tencent.com/developer/article/1936315">https://cloud.tencent.com/developer/article/1936315</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>『typora』相关配置与体验优化</title>
      <link href="/posts/c91a.html"/>
      <url>/posts/c91a.html</url>
      
        <content type="html"><![CDATA[<p>本文主要介绍typora配置过程中的一些<strong>问题的解决方法</strong>以及<strong>优化体验方案</strong>。</p><h1 id="版本过期报错"><a href="#版本过期报错" class="headerlink" title="版本过期报错"></a>版本过期报错</h1><p>下载完成后如果出现版本过期的报错：</p><p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20231204191844101.png" alt="image-20231204191844101"></p><p>具体解决方法参考：<a href="https://blog.csdn.net/wagnteng/article/details/126585618">https://blog.csdn.net/wagnteng/article/details/126585618</a></p><h1 id="设置右键菜单新建md"><a href="#设置右键菜单新建md" class="headerlink" title="设置右键菜单新建md"></a>设置右键菜单新建md</h1><p>任意位置创建一个txt文件，后缀改为reg</p><p>文件内容如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Windows Registry Editor Version 5.00</span><br><span class="line"> </span><br><span class="line">[HKEY_CLASSES_ROOT\.md]</span><br><span class="line">@=&quot;Typora.md&quot;</span><br><span class="line">&quot;Content Type&quot;=&quot;text/markdown&quot;</span><br><span class="line">&quot;PerceivedType&quot;=&quot;text&quot;</span><br><span class="line"> </span><br><span class="line">[HKEY_CLASSES_ROOT\.md\ShellNew]</span><br><span class="line">&quot;NullFile&quot;=&quot;&quot;</span><br></pre></td></tr></table></figure><p>完成后运行，运行完成后可以删除掉该文件</p><h1 id="图床配置：阿里云-PicGo-Core-command-line"><a href="#图床配置：阿里云-PicGo-Core-command-line" class="headerlink" title="图床配置：阿里云 + PicGo Core (command line)"></a>图床配置：阿里云 + PicGo Core (command line)</h1><p>Typora是一个跨平台的markdown编辑器，使用markdown编辑器写文章优点是排版简洁，痛点则是无法像Word那样直接嵌入图片，插入的图片实际上插入的是图片的路径（可以是本地路径也可以是网络路径），默认情况下为本地路径，这种情况下的md文档在别的终端上打开会无法查看图片。</p><p><strong>我们可以自己配置图床构建网络路径，来实现md文档的跨平台跨设备浏览</strong></p><p>下面介绍这里采用阿里云 + PicGo Core (command line)的方法来对图床进行配置的方法：</p><p><strong>1. PicGo Core (command line)下载与配置</strong></p><p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20230716132921821.png" alt="image-20230716132921821"></p><p>下载完成后打开找到文件夹 <code>C:\Users\用户名\AppData\Roaming\Typora\picgo\win64\</code> ，在对应文件夹下cmd执行 <code>./.config/Typora/picgo/linux/picgo install gitee-uploader</code>，若执行失败需要去安装nodejs环境</p><blockquote><p>nodejs安装教程：<a href="https://blog.csdn.net/zimeng303/article/details/112167688">https://blog.csdn.net/zimeng303/article/details/112167688</a></p></blockquote><p>完成后打开配置文件</p><p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/348e3092fc111d18984bd35b197a3357.png" alt="image-20200620214326041"></p><p>文件内容修改如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;picBed&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;uploader&quot;</span><span class="punctuation">:</span> <span class="string">&quot;aliyun&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;aliyun&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;accessKeyId&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;accessKeySecret&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;bucket&quot;</span><span class="punctuation">:</span> <span class="string">&quot;isodatop&quot;</span><span class="punctuation">,</span> <span class="comment">// 存储空间名</span></span><br><span class="line">      <span class="attr">&quot;area&quot;</span><span class="punctuation">:</span> <span class="string">&quot;oss-cn-beijing&quot;</span><span class="punctuation">,</span> <span class="comment">// 存储区域代号</span></span><br><span class="line">      <span class="attr">&quot;path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;img/&quot;</span><span class="punctuation">,</span> <span class="comment">// 自定义存储路径</span></span><br><span class="line">      <span class="attr">&quot;customUrl&quot;</span><span class="punctuation">:</span> <span class="string">&quot;https://isodatop.oss-cn-beijing.aliyuncs.com&quot;</span><span class="punctuation">,</span> <span class="comment">// 自定义域名，注意要加 http://或者 https://</span></span><br><span class="line">      <span class="attr">&quot;options&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span> <span class="comment">// 针对图片的一些后缀处理参数 PicGo 2.2.0+ PicGo-Core 1.4.0+</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;picgoPlugins&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;picgo-plugin-gitee-uploader&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><ul><li><p><strong>accesskeyid与secret的获取：</strong></p><p>登录阿里云，进入控制台，可以在里面查看这两个字段</p><p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20230716170158751.png" alt="image-20230716170158751"></p></li><li><p><strong>bucket、area、customUrl的获取</strong></p><p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20230716170626283.png" alt="image-20230716170626283"></p><p>再点击进入bucket，bucket名称即为isodatop</p><p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20230716170714631.png" alt="image-20230716170714631"></p><p>点击概览</p><p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20230716170833593.png" alt="image-20230716170833593"></p><p>这里的②为<code>oss-cn-beijing.aliyuncs.com</code>，注意要去掉后面的<code>.aliyuncs.com</code>，将<code>oss-cn-beijing</code>填入area字段</p><p>③为<code>isodatop.oss-cn-beijing.aliyuncs.com</code>，添加上<code>https://</code>后填入 customUrl 字段</p><p><strong>2. 测试</strong></p><p>配置完成后打开typora的偏好设置，验证上传服务是否配置成功</p><p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20230716171455885.png" alt="image-20230716171455885"></p><p>出现如下界面则上传成功：</p><p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20230716171616617.png" alt="image-20230716171616617"></p><blockquote><p><strong>如果出现报错或者上传成功但是图片在typora里面不予显示的情况</strong>：</p><p>注意检查前面几个字段是否按照要求进行了删除与添加相应的字段</p></blockquote></li></ul><h1 id="优化体验设置"><a href="#优化体验设置" class="headerlink" title="优化体验设置"></a>优化体验设置</h1><ul><li>文本高亮：<a href="https://blog.csdn.net/weixin_40626630/article/details/111405928">https://blog.csdn.net/weixin_40626630/article/details/111405928</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 资源工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> markdown </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
