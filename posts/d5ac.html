<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>『大模型』本地部署chatglm3-6b | isoda's Blog</title><meta name="author" content="isoda,673828733@qq.com"><meta name="copyright" content="isoda"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="在本地部署chatglm3-6b的工程记录">
<meta property="og:type" content="article">
<meta property="og:title" content="『大模型』本地部署chatglm3-6b">
<meta property="og:url" content="https://isoda.top/posts/d5ac.html">
<meta property="og:site_name" content="isoda&#39;s Blog">
<meta property="og:description" content="在本地部署chatglm3-6b的工程记录">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240122205804101.png">
<meta property="article:published_time" content="2024-01-23T11:52:55.589Z">
<meta property="article:modified_time" content="2024-04-02T06:24:54.456Z">
<meta property="article:author" content="isoda">
<meta property="article:tag" content="大模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240122205804101.png"><link rel="shortcut icon" href="https://isodatop.oss-cn-beijing.aliyuncs.com/img/SY7%7DZ%7B%28%28%7DTKN%29MJQGMMXJJ2.png"><link rel="canonical" href="https://isoda.top/posts/d5ac.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"本文距离发布日期已经过去了：","messageNext":"天，部分资源可能已经不存在了，详情可以咨询作者"},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体中文","cht_to_chs":"你已切换为简体中文","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#9400D3","bgDark":"#C0C0C0","position":"top-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '『大模型』本地部署chatglm3-6b',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-04-02 14:24:54'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.0.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/vv.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">26</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">30</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">12</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/life/"><i class="fa-fw fa fa-coffee"></i><span> 生活</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240122205804101.png')"><nav id="nav"><span id="blog-info"><a href="/" title="isoda's Blog"><span class="site-name">isoda's Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/life/"><i class="fa-fw fa fa-coffee"></i><span> 生活</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">『大模型』本地部署chatglm3-6b</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-01-23T11:52:55.589Z" title="发表于 2024-01-23 19:52:55">2024-01-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-02T06:24:54.456Z" title="更新于 2024-04-02 14:24:54">2024-04-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%B7%A5%E7%A8%8B%E8%AE%B0%E5%BD%95/">工程记录</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">1.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>6分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="『大模型』本地部署chatglm3-6b"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h1><p>ChatGLM3-6B发布于2023年10月27日，是目前中文能力排名最高的开源LLM模型。对应项目的github地址：<a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM3">https://github.com/THUDM/ChatGLM3</a></p>
<p>实验室项目中需要本地部署大模型用于新闻文本摘要任务，因为预测速度的要求及硬件条件的限制，比较后选择该模型的6B版本。</p>
<p>本文为在linux环境下的chatglm3-6b的本地部署工程记录。</p>
<h1 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h1><h2 id="CUDA与CUDNN"><a href="#CUDA与CUDNN" class="headerlink" title="CUDA与CUDNN"></a>CUDA与CUDNN</h2><p>深度学习任务中，当使用GPU进行训练的时候，需要安装英伟达提供的驱动和显卡对应的cuda、cudnn。</p>
<h3 id="什么是cuda和cudnn？"><a href="#什么是cuda和cudnn？" class="headerlink" title="什么是cuda和cudnn？"></a>什么是cuda和cudnn？</h3><ul>
<li>CUDA(ComputeUnified Device Architecture)：是一种由NVIDIA推出的通用并行计算架构，该架构使GPU能够解决复杂的计算问题。</li>
<li>CUDNN：是用于深度神经网络的GPU加速库。它强调性能、易用性和低内存开销。NVIDIA cuDNN可以集成到更高级别的机器学习框架中，如谷歌的Tensorflow、torch</li>
</ul>
<p>需要注意的是，cuda、cudnn和我们常用的深度学习框架（如tensorflow-gpu、torch-gpu）之间的版本存在对应关系，如果配置错误则代码不能运行。</p>
<p>我们首先查看服务器上的显卡相关信息，在命令行输入：</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure>

<p>可以看到显卡支持的最高CUDA版本为11.8（<strong>注意这里是显卡支持的最高CUDA版本，而并不是目前系统安装的CUDA版本</strong>！）</p>
<p>同时可以看到这台服务器上装载了两个显存大小都为24G的GPU 0和1，其中GPU 0的显存几乎已经完全占满。</p>
<p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240122191115501.png" alt="image-20240122191115501"></p>
<p>输入以下命令，查看是否已经安装了cuda：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc -V</span><br></pre></td></tr></table></figure>

<p>如果出现 <code>Command &#39;nvcc&#39; not found, but can be installed with:</code> 的输出，说明该服务器的系统并没有安装cuda</p>
<p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240122193539449.png" alt="image-20240122193539449"></p>
<p>可以看到我这台服务器上面已经安装了cuda，版本为11.8，所以并不需要进行cuda和cudnn的安装</p>
<p><strong>如果你和我一样已经安装了cuda和cudnn，请跳转到下一部分，进行相对应torch-gpu的安装</strong></p>
<h3 id="cuda和cudnn的安装"><a href="#cuda和cudnn的安装" class="headerlink" title="cuda和cudnn的安装"></a>cuda和cudnn的安装</h3><p>这一部分我并没有进行，考虑后续有没有机会补上。</p>
<p>首先要创建一个Python ≥ 3.10的虚拟环境（chatglm3-6b要求）。</p>
<p>要注意虚拟环境中的cuda和系统中的cuda的区别，我这里的cuda就是直接安装在系统中，所有的虚拟环境都可以使用系统中安装的cuda</p>
<p>在虚拟环境中安装并使用其他版本的cuda，参考：<a target="_blank" rel="noopener" href="https://blog.csdn.net/2301_80501457/article/details/134191613">https://blog.csdn.net/2301_80501457/article/details/134191613</a></p>
<blockquote>
<p>要注意：如果你的cuda是直接安装在虚拟环境中，并非直接安装在系统中。是无法使用ncvv -V命令验证是否安装成功的，ncvv -V是通过系统变量来直接查询，而安装在虚拟环境中并没有直接建立系统变量。验证方法可以见上面链接的最后一部分。</p>
</blockquote>
<h2 id="pytorch-gpu的安装"><a href="#pytorch-gpu的安装" class="headerlink" title="pytorch-gpu的安装"></a>pytorch-gpu的安装</h2><p>pytorch-gpu的版本同样必须依赖于CUDA的版本。</p>
<p>注意，<code>conda install pytorch</code> 命令安装的是torch CPU版本，但是我们要使用GPU进行训练</p>
<p>在<a target="_blank" rel="noopener" href="https://pytorch.org/get-started/previous-versions/">pytorch官网</a>上获取cuda11.8对应版本torch的安装命令</p>
<p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240122103348292.png" alt="image-20240122103348292"></p>
<p>复制对应的命令，执行安装</p>
<p>安装完成后编写程序如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="comment"># 如果pytorch安装成功即可导入</span></span><br><span class="line"><span class="built_in">print</span>(torch.cuda.is_available()) <span class="comment"># 查看CUDA是否可用</span></span><br><span class="line"><span class="built_in">print</span>(torch.cuda.device_count()) <span class="comment"># 查看可用的CUDA数量</span></span><br><span class="line"><span class="built_in">print</span>(torch.version.cuda) <span class="comment"># 查看CUDA版本</span></span><br></pre></td></tr></table></figure>

<p>运行结果如下：</p>
<p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240122195902702.png" alt="image-20240122195902702"></p>
<p>已经配置成功</p>
<h2 id="安装剩余依赖"><a href="#安装剩余依赖" class="headerlink" title="安装剩余依赖"></a>安装剩余依赖</h2><p>clone下来ChatGLM3项目</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/THUDM/ChatGLM3.git</span><br></pre></td></tr></table></figure>

<blockquote>
<p>超时的话挂梯子设置代理，使用服务器没办法挂梯子的话直接在github下载zip上传到服务器上</p>
</blockquote>
<p>由于已经安装了torch，所以我们要<strong>删掉项目目录下requirements.txt中的 <em>torch&gt;&#x3D;2.1.0</em> 一行</strong></p>
<p>并使用 pip 安装剩余的依赖</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>

<blockquote>
<p>这里一定要删除torch相关依赖行！！不然会自动给你下载冲突的torch版本，后续项目报错</p>
</blockquote>
<h1 id="下载ChatGLM3-6b模型及参数"><a href="#下载ChatGLM3-6b模型及参数" class="headerlink" title="下载ChatGLM3-6b模型及参数"></a>下载ChatGLM3-6b模型及参数</h1><h2 id="方法1：从-Hugging-Face-Hub-下载模型"><a href="#方法1：从-Hugging-Face-Hub-下载模型" class="headerlink" title="方法1：从 Hugging Face Hub 下载模型"></a>方法1：从 Hugging Face Hub 下载模型</h2><p>首先需要安装Git LFS，否则会出现模型中的大型文件下载不完整的情况。</p>
<p>输入以下命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git lfs install</span><br></pre></td></tr></table></figure>

<p>若显示 <code>Git LFS initialized</code> ，说明已经安装。</p>
<p>可以使用git进行克隆，运行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://huggingface.co/THUDM/chatglm3-6b</span><br></pre></td></tr></table></figure>

<p>如果下载比较慢或者超时，可以直接去<a target="_blank" rel="noopener" href="https://huggingface.co/THUDM/chatglm3-6b">官网</a>下载模型再上传到服务器</p>
<h2 id="方法2：从-ModelScope-下载模型（推荐）"><a href="#方法2：从-ModelScope-下载模型（推荐）" class="headerlink" title="方法2：从 ModelScope 下载模型（推荐）"></a>方法2：从 ModelScope 下载模型（推荐）</h2><p>同样也可以在国内魔搭社区下载，一般不会超时</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git lfs install</span><br><span class="line">git clone https://www.modelscope.cn/ZhipuAI/chatglm3-6b.git</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意，这里一定要<code>git lfs install</code>，虽然我查了这个命令仅仅只是用于验证是否安装了git lfs，但是我第一次从魔搭git clone的时候，没有使用这个命令产生了大文件下载不完整的情况，加上后没有再出现这样的问题，所以建议还是加上吧。</p>
</blockquote>
<h1 id="代码调用模型进行预测"><a href="#代码调用模型进行预测" class="headerlink" title="代码调用模型进行预测"></a>代码调用模型进行预测</h1><p>下载完成模型及参数文件后，我们将所有的文件放在一个文件夹<code>chatglm3-6b</code>里，然后将其放在项目文件夹ChatGLM3-main下</p>
<p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240122202232999.png" alt="image-20240122202232999"></p>
<p>然后我们可以在项目主目录下创建test.py文件，通过如下代码调用 ChatGLM 模型来生成对话：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModel</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;chatglm3-6b/&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModel.from_pretrained(<span class="string">&quot;chatglm3-6b/&quot;</span>, trust_remote_code=<span class="literal">True</span>, device=<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">model = model.<span class="built_in">eval</span>()</span><br><span class="line">response, history = model.chat(tokenizer, <span class="string">&quot;你好&quot;</span>, history=[])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;回答：&quot;</span>,response)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要根据上下文的情况下</span></span><br><span class="line"><span class="comment"># response, history = model.chat(tokenizer, &quot;晚上睡不着应该怎么办&quot;, history=history)</span></span><br><span class="line"><span class="comment"># print(response)</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>这里使用”chatglm3-6b&#x2F;“就是加载了该文件夹下的本地模型及文件</p>
</blockquote>
<p>在命令行输入，执行代码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=1 python test.py</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意，这里的 <code>CUDA_VISIBLE_DEVICES=1</code> 的作用是指定使用 GPU 1 来跑模型，因为我们前面已经通过 <code>nvidia-smi</code> 知道 GPU 0 的显存已经基本占满了</p>
<p>cuda指定GPU、设置多GPU的方法：<a target="_blank" rel="noopener" href="https://blog.csdn.net/OneQuestionADay/article/details/111691486">https://blog.csdn.net/OneQuestionADay/article/details/111691486</a></p>
</blockquote>
<h1 id="构建网页版demo"><a href="#构建网页版demo" class="headerlink" title="构建网页版demo"></a>构建网页版demo</h1><p>官方教程：<a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM3/blob/main/composite_demo/README.md">https://github.com/THUDM/ChatGLM3/blob/main/composite_demo/README.md</a></p>
<p>安装 Jupyter 内核：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ipython kernel install --name chatglm3-demo --user</span><br></pre></td></tr></table></figure>

<p>因为我们是本地下载的模型，所以需要先设置环境变量 <code>MODEL_PATH</code> 来指定从本地加载模型</p>
<p>直接在终端中输入如下命令即可</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export MODEL_PATH=//home/xjb/event/ChatGLM3-main/chatglm3-6b</span><br></pre></td></tr></table></figure>

<p>然后输入命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=1 streamlit run composite_demo/main.py</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意这里一样要在命令的前面加上 <code>CUDA_VISIBLE_DEVICES=1</code> 来设置使用的GPU</p>
</blockquote>
<p>运行结果：</p>
<p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240122204008879.png" alt="image-20240122204008879"></p>
<p>如果你部署在了服务器上而不是本机，访问对应的network url既可使用网页demo</p>
<p><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240122204313569.png" alt="image-20240122204313569"></p>
<p>现在你就拥有一个属于自己的本地大模型啦，因为该模型较轻量级，预测的速度还是比较快的，在未经过微调的情况下我直接将其用于新闻文本的摘要任务，大概1-2秒可以生成一篇文本的摘要，效果也比较不错，但是偶尔会出现中英文乱码的情况。</p>
<p>你也可以对其进行进一步的领域微调，使其更适合你的任务。</p>
<h1 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h1><ul>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_43705697/article/details/121618276">https://blog.csdn.net/qq_43705697/article/details/121618276</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/2301_80501457/article/details/134191613">https://blog.csdn.net/2301_80501457/article/details/134191613</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_65814643/article/details/134510135?spm=1001.2014.3001.5501">https://blog.csdn.net/m0_65814643/article/details/134510135?spm=1001.2014.3001.5501</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_65814643/article/details/134563092">https://blog.csdn.net/m0_65814643/article/details/134563092</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/OneQuestionADay/article/details/111691486">https://blog.csdn.net/OneQuestionADay/article/details/111691486</a></p>
</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://isoda.top">isoda</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://isoda.top/posts/d5ac.html">https://isoda.top/posts/d5ac.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://isoda.top" target="_blank">isoda's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a></div><div class="post_share"><div class="social-share" data-image="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240122205804101.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://isodatop.oss-cn-beijing.aliyuncs.com/img/QQ%E6%88%AA%E5%9B%BE20231208115838.png" target="_blank"><img class="post-qr-code-img" src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/QQ%E6%88%AA%E5%9B%BE20231208115838.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/77ff.html" title="『浏览器安全』浏览器同源策略与沙箱"><img class="cover" src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/R.f54d1cf1d1e34b8bca603a4f94de3b88" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">『浏览器安全』浏览器同源策略与沙箱</div></div></a></div><div class="next-post pull-right"><a href="/posts/3bee.html" title="『linux』ubuntu修改python版本"><img class="cover" src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/wp9104504.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">『linux』ubuntu修改python版本</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/vv.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">isoda</div><div class="author-info__description">简单 自律 高效</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">26</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">30</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">12</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/soda-VV"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/soda-VV" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:673828733@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">这里是isoda的个人博客，在这里分享我的学习与生活，致力于毕业当公务员摆烂，有问题欢迎探讨交流</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#motivation"><span class="toc-number">1.</span> <span class="toc-text">motivation</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE"><span class="toc-number">2.</span> <span class="toc-text">环境配置</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#CUDA%E4%B8%8ECUDNN"><span class="toc-number">2.1.</span> <span class="toc-text">CUDA与CUDNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFcuda%E5%92%8Ccudnn%EF%BC%9F"><span class="toc-number">2.1.1.</span> <span class="toc-text">什么是cuda和cudnn？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cuda%E5%92%8Ccudnn%E7%9A%84%E5%AE%89%E8%A3%85"><span class="toc-number">2.1.2.</span> <span class="toc-text">cuda和cudnn的安装</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch-gpu%E7%9A%84%E5%AE%89%E8%A3%85"><span class="toc-number">2.2.</span> <span class="toc-text">pytorch-gpu的安装</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E5%89%A9%E4%BD%99%E4%BE%9D%E8%B5%96"><span class="toc-number">2.3.</span> <span class="toc-text">安装剩余依赖</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BDChatGLM3-6b%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%8F%82%E6%95%B0"><span class="toc-number">3.</span> <span class="toc-text">下载ChatGLM3-6b模型及参数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B9%E6%B3%951%EF%BC%9A%E4%BB%8E-Hugging-Face-Hub-%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.1.</span> <span class="toc-text">方法1：从 Hugging Face Hub 下载模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B9%E6%B3%952%EF%BC%9A%E4%BB%8E-ModelScope-%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B%EF%BC%88%E6%8E%A8%E8%8D%90%EF%BC%89"><span class="toc-number">3.2.</span> <span class="toc-text">方法2：从 ModelScope 下载模型（推荐）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E8%B0%83%E7%94%A8%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E9%A2%84%E6%B5%8B"><span class="toc-number">4.</span> <span class="toc-text">代码调用模型进行预测</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E7%BD%91%E9%A1%B5%E7%89%88demo"><span class="toc-number">5.</span> <span class="toc-text">构建网页版demo</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%AB%A0"><span class="toc-number">6.</span> <span class="toc-text">参考文章</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/a395.html" title="『注入攻击』Injection"><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/3908b96858be3a17bc39c6f3254d5a1100d2d71a.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="『注入攻击』Injection"/></a><div class="content"><a class="title" href="/posts/a395.html" title="『注入攻击』Injection">『注入攻击』Injection</a><time datetime="2024-09-15T07:40:24.000Z" title="发表于 2024-09-15 15:40:24">2024-09-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/4981.html" title="『文件上传漏洞』File Upload Vulnerabilities"><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/9a3eaa168ddeaaaaa656d430d0ae4678.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="『文件上传漏洞』File Upload Vulnerabilities"/></a><div class="content"><a class="title" href="/posts/4981.html" title="『文件上传漏洞』File Upload Vulnerabilities">『文件上传漏洞』File Upload Vulnerabilities</a><time datetime="2024-09-01T02:35:44.000Z" title="发表于 2024-09-01 10:35:44">2024-09-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/9dd6.html" title="『Docker』Docker基础知识"><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/docker.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="『Docker』Docker基础知识"/></a><div class="content"><a class="title" href="/posts/9dd6.html" title="『Docker』Docker基础知识">『Docker』Docker基础知识</a><time datetime="2024-08-15T01:30:13.000Z" title="发表于 2024-08-15 09:30:13">2024-08-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/c2a6.html" title="『Java内存马』RASP检测Java内存马"><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20241206104623582.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="『Java内存马』RASP检测Java内存马"/></a><div class="content"><a class="title" href="/posts/c2a6.html" title="『Java内存马』RASP检测Java内存马">『Java内存马』RASP检测Java内存马</a><time datetime="2024-08-01T00:25:43.000Z" title="发表于 2024-08-01 08:25:43">2024-08-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/64ed.html" title="『web前端开发』React快速入门"><img src="https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20241206154646938.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="『web前端开发』React快速入门"/></a><div class="content"><a class="title" href="/posts/64ed.html" title="『web前端开发』React快速入门">『web前端开发』React快速入门</a><time datetime="2024-07-25T00:23:14.000Z" title="发表于 2024-07-25 08:23:14">2024-07-25</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://isodatop.oss-cn-beijing.aliyuncs.com/img/image-20240122205804101.png')"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2025 By isoda</div><div class="footer_custom_text">Hi, welcome to my <a target="_blank" rel="noopener" href="https://butterfly.js.org/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>